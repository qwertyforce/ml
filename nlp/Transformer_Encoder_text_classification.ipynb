{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# some commands in th is notebook require torchtext 0.12.0\n",
        "# !pip install  torchtext --upgrade --quiet\n",
        "# !pip install torchdata --quiet\n",
        "# !pip install torchinfo --quiet"
      ],
      "metadata": {
        "id": "Mtq3abS2lL_i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "import torchtext\n",
        "import torchdata\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torchinfo\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "id": "bQEBuaIm5s9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364055db-7867-4a96-89ad-f416c44fb2d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ],
      "metadata": {
        "id": "SmM0VcYnCC8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB\n",
        "train_iter, test_iter = IMDB()\n",
        "num_classes = len(set([label for (label, text) in train_iter]))\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "7UdYsN6J5uke"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see an example of the dateset\n",
        "next(iter(train_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HchVgEXWlqLz",
        "outputId": "2ed36cd5-4e0b-4a5e-de99-b6e07deb9e3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "# convert the labels to be in range(0, num_classes)\n",
        "y_train = torch.tensor([label-1 for (label, text) in train_iter])\n",
        "y_test  = torch.tensor([label-1 for (label, text) in test_iter])\n",
        "\n",
        "# tokenize the texts, and truncate the number of words in each text to max_seq_len\n",
        "max_seq_len = 512\n",
        "x_train_texts = [tokenizer(text.lower())[0:max_seq_len]\n",
        "                 for (label, text) in train_iter]\n",
        "x_test_texts  = [tokenizer(text.lower())[0:max_seq_len]\n",
        "                 for (label, text) in test_iter]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6DeTWUptkYnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "# build the vocabulary and word-to-integer map\n",
        "counter = collections.Counter()\n",
        "for text in x_train_texts:\n",
        "    counter.update(text)\n",
        "\n",
        "vocab_size = 95833\n",
        "most_common_words = np.array(counter.most_common(vocab_size - 2))\n",
        "vocab = most_common_words[:,0]\n",
        "\n",
        "# indexes for the padding token, and unknown tokens\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "word_to_id = {vocab[i]: i + 2 for i in range(len(vocab))}"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MYVE8HSGkYnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "# map the words in the training and test texts to integers\n",
        "x_train = [torch.tensor([word_to_id.get(word, UNK) for word in text],dtype=torch.int32)\n",
        "           for text in x_train_texts]\n",
        "x_test  = [torch.tensor([word_to_id.get(word, UNK) for word in text],dtype=torch.int32)\n",
        "          for text in x_test_texts]\n",
        "x_train = torch.nn.utils.rnn.pad_sequence(x_train,batch_first=True, padding_value = PAD)\n",
        "x_test = torch.nn.utils.rnn.pad_sequence(x_test,batch_first=True, padding_value = PAD)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "I7-4KQI8kYnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7RUYnQ6m6ph",
        "outputId": "5377ef6e-40a0-4320-e582-9c05284d8b51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25000, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "# constructing the dataset in order to be compatible with torch.utils.data.Dataloader\n",
        "class IMDBDataset:\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.features[item], self.labels[item]\n",
        "\n",
        "\n",
        "train_dataset = IMDBDataset(x_train, y_train)\n",
        "test_dataset  = IMDBDataset(x_test, y_test)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hJe8LAUNkYnI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "def collate_stack_fn(batch):\n",
        "    xx, yy = zip(*batch)\n",
        "    return  torch.stack(xx), torch.stack(yy)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn = collate_stack_fn)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn = collate_stack_fn)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ov3tX4sRkYnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the encoder-only transformer model for text classification"
      ],
      "metadata": {
        "id": "zNVoCKz0CM3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        n_heads, d_embed, dropout_attn, dropout_proj, device = config.n_heads, config.d_embed, config.dropout_attn, config.dropout_proj, config.device\n",
        "\n",
        "        assert d_embed % n_heads == 0        \n",
        "        self.hid_dim = d_embed\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_embed // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(d_embed, d_embed)\n",
        "        self.fc_k = nn.Linear(d_embed, d_embed)\n",
        "        self.fc_v = nn.Linear(d_embed, d_embed)\n",
        "        self.proj = nn.Linear(d_embed, d_embed)\n",
        "        \n",
        "        self.dropout_attn = nn.Dropout(dropout_attn)\n",
        "        self.dropout_proj = nn.Dropout(dropout_proj)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, x, mask = None):       \n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        Q,K,V = self.fc_q(x), self.fc_k(x), self.fc_v(x) #[batch size, query len, hid dim\n",
        "    \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch size, n heads, query len, head dim]\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)  \n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) \n",
        "                \n",
        "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) /  self.scale  #attention = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        attention = self.dropout_attn(torch.softmax(attention, dim = -1)) # Attention Dropout;  attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(attention, V) #x = [batch size, n heads, query len, head dim]\n",
        "        x = x.permute(0, 2, 1, 3).contiguous() #x = [batch size, query len, n heads, head dim]\n",
        "        x = x.view(batch_size, -1, self.hid_dim) #x = [batch size, query len, hid dim] (hidden concat of all heads)\n",
        "        x = self.dropout_proj(self.proj(x)) #x = [batch size, query len, hid dim]\n",
        "\n",
        "        return x #,attention\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.tok_embed = nn.Embedding(config.enc_vocab_size, config.d_embed)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, config.context_len, config.d_embed))\n",
        "        self.enc_block = nn.ModuleList([EncoderBlock(config) for _ in range(config.enc_blocks)])\n",
        "        self.dropout_pos = nn.Dropout(config.dropout_pos)\n",
        "        self.norm = nn.LayerNorm(config.d_embed)\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "        x = self.tok_embed(input)\n",
        "        x_pos = self.pos_embed[:, :x.size(1), :]\n",
        "        x = self.dropout_pos(x + x_pos)\n",
        "        for layer in self.enc_block:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        \n",
        "        self.mhsa = MultiHeadedSelfAttention(config)\n",
        "        self.pwff = nn.Sequential(\n",
        "            nn.Linear(config.d_embed, config.d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.d_ff, config.d_embed),\n",
        "            nn.Dropout(config.dropout_pwff)\n",
        "        )\n",
        "        self.ln_mhsa = nn.LayerNorm(config.d_embed)\n",
        "        self.ln_pwff = nn.LayerNorm(config.d_embed)\n",
        "        self.dropout = nn.Dropout(config.dropout_res)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.dropout(self.mhsa(self.ln_mhsa(x)))   # Pre-LN\n",
        "        x = x + self.dropout(self.pwff(self.ln_pwff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, num_classes):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(config)\n",
        "        self.linear = nn.Linear(config.d_embed, num_classes)\n",
        "\n",
        "    def forward(self, x, pad_mask=None):\n",
        "        x = self.encoder(x, pad_mask)\n",
        "        return  self.linear(torch.mean(x,-2))"
      ],
      "metadata": {
        "id": "5ZyfPnM-qSUK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    enc_vocab_size: int\n",
        "    d_embed: int\n",
        "    d_ff: int\n",
        "    n_heads: int\n",
        "    enc_blocks: int\n",
        "    context_len: int\n",
        "    dropout_res: float\n",
        "    dropout_attn: float\n",
        "    dropout_proj: float\n",
        "    dropout_pos: float\n",
        "    dropout_pwff: float\n",
        "    device: str\n",
        "  \n",
        "def make_model(config):\n",
        "    model = Transformer(config, num_classes).to(DEVICE)\n",
        "    # initialize model parameters\n",
        "    # it seems that this initialization is very important!\n",
        "    for p in model.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "spXtU6nVqWfy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "1HdgZPJBoKY2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    losses, acc, count = [], 0, 0\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for idx, (x, y)  in  pbar:\n",
        "        optimizer.zero_grad()\n",
        "        features= x.to(DEVICE)\n",
        "        labels  = y.to(DEVICE)\n",
        "        pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
        "        pred = model(features, pad_mask)\n",
        "\n",
        "        loss = loss_fn(pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        acc += (pred.argmax(1) == labels).sum().item()\n",
        "        count += len(labels)\n",
        "        # report progress\n",
        "        if idx>0 and idx%50 == 0:\n",
        "            pbar.set_description(f'train loss={loss.item():.4f}, train_acc={acc/count:.4f}')\n",
        "    return np.mean(losses), acc/count\n",
        "\n",
        "def train(model, train_loader, test_loader, epochs):\n",
        "    for ep in range(epochs):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader)\n",
        "        val_loss, val_acc = evaluate(model, test_loader)\n",
        "        print(f'ep {ep}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')\n",
        "        \n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    losses, acc, count = [], 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(dataloader):\n",
        "            features = x.to(DEVICE)\n",
        "            labels  = y.to(DEVICE)\n",
        "            pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
        "            pred = model(features, pad_mask)\n",
        "            loss = loss_fn(pred,labels).to(\"cpu\")\n",
        "            losses.append(loss.item())\n",
        "            acc += (pred.argmax(1) == labels).sum().item()\n",
        "            count += len(labels)\n",
        "    return np.mean(losses), acc/count"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ydp6IfBrkYnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in tqdm(test_loader):\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ww8G_XGfuU9",
        "outputId": "fc179d1e-1923-4f51-ed57-92f237ae9d95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [00:00<00:00, 1266.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelConfig(enc_vocab_size = vocab_size,\n",
        "                     d_embed = 32,\n",
        "                     d_ff = 4*32,\n",
        "                     n_heads = 4,\n",
        "                     enc_blocks = 1,\n",
        "                     context_len = max_seq_len,\n",
        "                     dropout_res = 0.1,\n",
        "                     dropout_attn = 0.1,\n",
        "                     dropout_proj= 0.1,\n",
        "                     dropout_pos=  0.1,\n",
        "                     dropout_pwff = 0.1,\n",
        "                     device=\"cuda\"\n",
        "                     )"
      ],
      "metadata": {
        "id": "CCC5cnaMquid"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "Layer (type:depth-idx)                             Param #\n",
            "===========================================================================\n",
            "Transformer                                        --\n",
            "├─Encoder: 1-1                                     16,384\n",
            "│    └─Embedding: 2-1                              3,066,656\n",
            "│    └─ModuleList: 2-2                             --\n",
            "│    │    └─EncoderBlock: 3-1                      12,704\n",
            "│    └─Dropout: 2-3                                --\n",
            "│    └─LayerNorm: 2-4                              64\n",
            "├─Linear: 1-2                                      66\n",
            "===========================================================================\n",
            "Total params: 3,095,874\n",
            "Trainable params: 3,095,874\n",
            "Non-trainable params: 0\n",
            "===========================================================================\n"
          ]
        }
      ],
      "source": [
        "model = make_model(config)\n",
        "print(torchinfo.summary(model))\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIau66WRlzRm",
        "outputId": "9a03cda6-390e-4fad-89f0-1758f2b5cc2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "7FZF7veDrE1j",
        "outputId": "7dbb6656-7aa4-44ee-e696-b0a938eaddeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embed): Embedding(95833, 32)\n",
              "    (enc_block): ModuleList(\n",
              "      (0): EncoderBlock(\n",
              "        (mhsa): MultiHeadedSelfAttention(\n",
              "          (fc_q): Linear(in_features=32, out_features=32, bias=True)\n",
              "          (fc_k): Linear(in_features=32, out_features=32, bias=True)\n",
              "          (fc_v): Linear(in_features=32, out_features=32, bias=True)\n",
              "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "          (dropout_attn): Dropout(p=0.1, inplace=False)\n",
              "          (dropout_proj): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pwff): Sequential(\n",
              "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
              "          (3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_mhsa): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (ln_pwff): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout_pos): Dropout(p=0.1, inplace=False)\n",
              "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (linear): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train loss=0.4075, train_acc=0.7828: 100%|██████████| 196/196 [00:14<00:00, 13.14it/s]\n",
            "100%|██████████| 196/196 [00:04<00:00, 45.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0: val_loss=0.2996, val_acc=0.8790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train loss=0.1572, train_acc=0.9618: 100%|██████████| 196/196 [00:14<00:00, 13.64it/s]\n",
            "100%|██████████| 196/196 [00:04<00:00, 44.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1: val_loss=0.4848, val_acc=0.8582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train loss=0.0070, train_acc=0.9928: 100%|██████████| 196/196 [00:14<00:00, 13.51it/s]\n",
            "100%|██████████| 196/196 [00:04<00:00, 43.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2: val_loss=0.5999, val_acc=0.8489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train loss=0.0010, train_acc=0.9978: 100%|██████████| 196/196 [00:14<00:00, 13.32it/s]\n",
            "100%|██████████| 196/196 [00:04<00:00, 42.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3: val_loss=0.7828, val_acc=0.8436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(model, train_loader, test_loader, epochs=4)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo7RYNx0lzRm",
        "outputId": "a5c31727-6b2e-49e5-8afa-49715148280d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_label = [\"neg\",\"pos\"]\n",
        "\n",
        "def classify_review(news):\n",
        "    x_text = tokenizer(news.lower())[0:max_seq_len]\n",
        "    x_int = torch.tensor([[word_to_id.get(word, UNK) for word in x_text]]).to(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model(x_int).argmax(1).item()\n",
        "    print(f\"This is a {review_label[pred]} review\")\n",
        "\n",
        "# The model correctly classifies a theoretical physics news as Sci/Tec news, :-)\n",
        "review_pos = \"\"\"Confidently directed, dark, brooding, and packed with impressive action sequences and a complex story,\n",
        " The Dark Knight includes a career-defining turn from Heath Ledger as well as other Oscar worthy performances,\n",
        "  TDK remains not only the best Batman movie, but comic book movie ever created.\n",
        "\"\"\"\n",
        "classify_review(review_pos)\n",
        "\n",
        "\n",
        "review_neg = \"\"\"Plot holes the size of the grand canyon,\n",
        " overall terrible acting, and about 45 minutes of useless fluff at the end.\n",
        " Had this been the real world,\n",
        " The Joker would have been caught about 15 minutes into the movie and the credits would have rolled.\n",
        " Too bad that didn't happen.\n",
        "\"\"\"\n",
        "classify_review(review_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh-H72P81ApT",
        "outputId": "7143d05e-d10e-439b-b24e-5146f0eabed6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a pos review\n",
            "This is a neg review\n"
          ]
        }
      ]
    }
  ]
}