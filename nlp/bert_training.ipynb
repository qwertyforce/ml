{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-16 20:45:37.900467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 20:45:38.688392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./pr_ml_nd_cpe_and_access_vector.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "all_data = [ [el[0],[el[1:]]] for el in data.values.tolist()]\n",
    "random.shuffle(all_data)\n",
    "X=[el[0] for el in all_data]\n",
    "y = [el[1][0] for el in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(tokenizer(X, padding=\"max_length\",truncation=True,max_length=192)[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_4.npy', 'wb') as f:\n",
    "#     np.save(f, X)\n",
    "# with open('y_4.npy', 'wb') as f:\n",
    "#     np.save(f, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_1.npy', 'rb') as f:\n",
    "#     X = np.load(f)\n",
    "# with open('y_1.npy', 'rb') as f:\n",
    "#     y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train,np.float32)\n",
    "y_test  = np.array(y_test,np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# X_train_cuda = torch.from_numpy(X_train).cuda()\n",
    "# X_test_cuda = torch.from_numpy(X_test).cuda()\n",
    "# y_test_cuda = torch.from_numpy(y_test).cuda()\n",
    "# y_train_cuda = torch.from_numpy(y_train).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, AutoConfig\n",
    "configuration = AutoConfig.from_pretrained('roberta-base')\n",
    "configuration.hidden_dropout_prob = 0.3\n",
    "configuration.attention_probs_dropout_prob = 0.3\n",
    "configuration.num_labels=4\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config=configuration)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", config=configuration)\n",
    "model=model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokens,labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            return (tokens, label)\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "BATCH_SIZE=42\n",
    "train_dataset = CustomDataset(X_train,y_train)\n",
    "test_dataset = CustomDataset(X_test,y_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True,pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4399 1467\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader),len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from statistics import mean\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch import autocast\n",
    "# import optuna\n",
    "# import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "device=\"cuda\"\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(lr,trial=None,EPOCHS=5):\n",
    "    # wandb.init(project=\"bert_test\", entity=\"qwertyforce\",reinit=True,name=\"123_test\")\n",
    "\n",
    "    # wandb.config.update({\n",
    "    #     \"learning_rate\": lr,\n",
    "    #     \"epochs\": EPOCHS,\n",
    "    #     \"batch_size\": BATCH_SIZE,\n",
    "    #     })\n",
    "    wandb=None\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr) \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    loss_train=[]\n",
    "    loss_test=[]\n",
    "    acc_test=[]\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        train_loss = train_one_epoch(optimizer,criterion,scaler)\n",
    "        loss_train.append(train_loss)\n",
    "\n",
    "        test_loss,test_acc = test(criterion)\n",
    "        loss_test.append(test_loss)\n",
    "        acc_test.append(test_acc)\n",
    "\n",
    "        if wandb:\n",
    "            wandb.log({\"loss_train\": loss_train[-1],\"epoch\":epoch})\n",
    "            wandb.log({\"loss_test\": loss_test[-1],\"epoch\":epoch})\n",
    "            wandb.log({\"acc_test\": acc_test[-1],\"epoch\":epoch})\n",
    "        # if trial:\n",
    "        #     trial.report(mean(loss_test), epoch)\n",
    "        #     if trial.should_prune():\n",
    "        #         raise optuna.exceptions.TrialPruned()\n",
    "    return loss_train,loss_test,acc_test\n",
    "\n",
    "\n",
    "def train_one_epoch(optimizer,criterion,scaler):\n",
    "    model.train()\n",
    "    temp_train_loss=[]\n",
    "    start = timer()\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        # print(data.shape)\n",
    "        # labels=labels.reshape(-1)\n",
    "        # labels = labels.squeeze()\n",
    "        # print(labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model.forward(data)[\"logits\"]\n",
    "            # print(outputs.dtype)\n",
    "            # print(labels.dtype)\n",
    "            loss = criterion(outputs,labels)\n",
    "        \n",
    "        # outputs = model.forward(data)\n",
    "        # loss = criterion(outputs,labels.unsqueeze(1))\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        temp_train_loss.append(loss.item())\n",
    "        if batch_idx % 100==0:\n",
    "            if wandb:\n",
    "                wandb.log({\"loss_train_batch_idx\": mean(temp_train_loss[-100:]),\"batch_idx\":batch_idx})\n",
    "\n",
    "        if batch_idx % 500==0:\n",
    "            if wandb:\n",
    "                wandb.log({\"loss_test_batch_idx\": test(criterion,partial=True),\"batch_idx\":batch_idx})\n",
    "                model.train()\n",
    "            \n",
    "    end = timer()\n",
    "    train_loss = mean(temp_train_loss)\n",
    "    print(f\"Train loss = {train_loss}; epoch_training_time: {end - start}\")\n",
    "    return train_loss\n",
    "\n",
    "            \n",
    "\n",
    "def test(criterion,partial=False):\n",
    "    model.eval()\n",
    "    temp_loss=[]\n",
    "    predictions, true_labels = [], []\n",
    "    for batch_idx, (data, labels) in tqdm(enumerate(test_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            # labels = labels.squeeze()\n",
    "           \n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # labels=labels.reshape(-1)\n",
    "            outputs = model.forward(data)[\"logits\"]\n",
    "            print(outputs.device)\n",
    "            print(labels.device)\n",
    "            loss = criterion(outputs,labels)\n",
    "            temp_loss.append(loss.item())\n",
    "              #   print(outputs)\n",
    "             #   break\n",
    "            outputs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            outputs = np.round(outputs)\n",
    "            # outputs=outputs.cpu().numpy()\n",
    "            # outputs=np.argmax(outputs,axis=1)\n",
    "            # outputs = np.round(outputs)\n",
    "            # print(outputs)\n",
    "            predictions.extend(outputs)\n",
    "            if partial and batch_idx == 100:\n",
    "                return mean(temp_loss)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels,predictions)\n",
    "    test_loss = mean(temp_loss)\n",
    "    print(f\"Test loss = {test_loss}; Test acc = {accuracy}\")\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(0.00002,EPOCHS=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=[]\n",
    "# b=[]\n",
    "# test(a,b,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963it [06:58,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10316330983921468\n",
      "0.893150595953363\n",
      "Test: 0.10316330983921468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "temp_loss=[]\n",
    "predictions, true_labels = [], []\n",
    "for batch_idx, (data, labels) in tqdm(enumerate(test_dataloader)):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        # labels=labels.reshape(-1)\n",
    "        outputs = model.forward(data)[\"logits\"]\n",
    "        loss = criterion(outputs,labels)\n",
    "        temp_loss.append(loss.item())\n",
    "    #   print(outputs)\n",
    "    #   break\n",
    "        outputs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        outputs = np.round(outputs)\n",
    "        # outputs=outputs.cpu().numpy()\n",
    "        # outputs=np.argmax(outputs,axis=1)\n",
    "        # print(outputs)\n",
    "        predictions.extend(outputs)\n",
    "        # if batch_idx == 10:\n",
    "        #     break\n",
    "\n",
    "\n",
    "print(mean(temp_loss))\n",
    "print(accuracy_score(true_labels,predictions))\n",
    "print(f\"Test: {mean(temp_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr=0\n",
    "# for i in range(len(predictions)):\n",
    "#     if (true_labels[i]==predictions[i]).all():\n",
    "#         rr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rrr=0\n",
    "# for i in range(len(predictions)):\n",
    "#     if true_labels[i][9]==predictions[i][9]:\n",
    "#         rrr+=1\n",
    "# print(rrr/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     54281\n",
      "           1       0.87      0.81      0.84      3337\n",
      "           2       0.82      0.84      0.83     13708\n",
      "           3       0.75      0.66      0.70       671\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     71997\n",
      "   macro avg       0.85      0.82      0.84     71997\n",
      "weighted avg       0.94      0.94      0.94     71997\n",
      " samples avg       0.95      0.96      0.95     71997\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qwertyforce/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
