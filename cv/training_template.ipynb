{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# from tqdm import tqdm\n",
    "# import shutil \n",
    "# import random\n",
    "\n",
    "# image_paths_and_classes = []\n",
    "\n",
    "# for file_name in os.listdir(\"./train_generated/no_watermark/\"):\n",
    "#     image_paths_and_classes.append((\"./train_generated/no_watermark/\"+file_name,\"no_watermark\", file_name))\n",
    "\n",
    "# for file_name in os.listdir(\"./train_generated/watermark/\"):\n",
    "#     image_paths_and_classes.append((\"./train_generated/watermark/\"+file_name,\"watermark\", file_name))\n",
    "\n",
    "# random.shuffle(image_paths_and_classes)\n",
    "# testing = image_paths_and_classes[int(len(image_paths_and_classes)*0.8):]   #0.2\n",
    "\n",
    "# for obj in testing:\n",
    "#     shutil.move(obj[0],f\"./train_generated/test/{obj[1]}/\"+obj[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# from tqdm import tqdm\n",
    "# from PIL import Image\n",
    "# deleted=0\n",
    "# paths= [\"./train_generated/watermark/\",\"./train_generated/no_watermark/\"]\n",
    "# for p in paths:\n",
    "#     for file_name in tqdm(os.listdir(p)):\n",
    "#         try:\n",
    "#             img = Image.open(p+file_name)\n",
    "#             img.load()\n",
    "#         except:\n",
    "#             os.remove(p+file_name)\n",
    "#             deleted+=1\n",
    "# print(deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# from tqdm import tqdm\n",
    "# from PIL import Image\n",
    "# deleted=0\n",
    "# paths= [\"./train_generated/watermark/\",\"./train_generated/no_watermark/\",\"./watermark/\",\"./no_watermark/\"]\n",
    "# for p in paths:\n",
    "#     for file_name in tqdm(os.listdir(p)):\n",
    "#         if \"_aug_\" in file_name:\n",
    "#             os.remove(p+file_name)\n",
    "#             deleted+=1\n",
    "# print(deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# from tqdm import tqdm\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# deleted=0\n",
    "# paths= [\"./train_generated/watermark/\",\"./train_generated/no_watermark/\"]\n",
    "# for p in paths:\n",
    "#     for file_name in tqdm(os.listdir(p)):\n",
    "#         if \"_aug_\" in file_name:\n",
    "#             # os.remove(p+file_name)\n",
    "#             # deleted+=1\n",
    "#             pass\n",
    "#         else:\n",
    "#             img = Image.open(p+file_name)\n",
    "#             if img.size[0]!=512 or img.size[1]!=512:\n",
    "#                 img=img.resize((512,512),Image.Resampling(np.random.randint(1,6)))    #[0,5)\n",
    "#                 img.save(p+file_name,quality=90)\n",
    "# # print(deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import albumentations as A\n",
    "# import numpy as np\n",
    "# all_transforms = [\n",
    "#   A.Compose([\n",
    "#     A.augmentations.geometric.rotate.SafeRotate(limit=30,value=[0,0,0], border_mode=0, p=0.10),\n",
    "#     A.core.composition.OneOf([A.Lambda(p=1), A.VerticalFlip(p=1), A.HorizontalFlip(p=1),A.RandomRotate90(p=1)],p=1),\n",
    "#     A.core.composition.OneOf([A.Lambda(p=1), A.augmentations.ToGray(p=1), A.augmentations.RGBShift(p=1),\n",
    "#     A.augmentations.ChannelShuffle(p=1),A.augmentations.ColorJitter(p=1),A.InvertImg(p=1)],p=1),\n",
    "#     A.core.composition.OneOf([A.Lambda(p=1),A.augmentations.MotionBlur(p=1), A.augmentations.AdvancedBlur(p=1), A.augmentations.GaussianBlur(p=1)],p=1),\n",
    "#     A.ImageCompression(p=0.20, quality_lower=35),\n",
    "# ])\n",
    "# ]\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from PIL import Image\n",
    "# paths= [\"./train_generated/watermark/\",\"./train_generated/no_watermark/\",\"./no_watermark/\",\"./watermark/\"]\n",
    "# for p in paths:\n",
    "#     for file_name in tqdm(os.listdir(p)):\n",
    "#           img = Image.open(p+file_name)\n",
    "#           if img.mode != 'RGB':\n",
    "#             img = img.convert('RGB')\n",
    "#           # img=img.resize((512,512),Image.Resampling(np.random.randint(6)))    #[0,5)\n",
    "#           img = np.array(img)\n",
    "#           for i in range(1):\n",
    "#             new_img = all_transforms[0](image=img)[\"image\"]\n",
    "#             # print(new_img.shape)\n",
    "#             new_img =  Image.fromarray(new_img,\"RGB\")\n",
    "\n",
    "#             new_file_name = f'{file_name[:file_name.index(\".\")]}_aug_{str(i)}_{file_name[file_name.index(\".\"):]}'\n",
    "#             new_img.save(p+new_file_name,quality=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "device=\"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_model():\n",
    "    model = timm.create_model('tf_efficientnetv2_b0', pretrained=True,drop_path_rate=0.2,drop_rate=0.2)\n",
    "    model.classifier=torch.nn.Linear(1280,1)\n",
    "\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # for blk in [model.classifier, model.global_pool,model.conv_head,model.blocks[-1],model.blocks[-2],model.blocks[-3]]:\n",
    "    #     for param in blk.parameters():\n",
    "    #         param.requires_grad = True\n",
    "\n",
    "    # for module in model.modules():\n",
    "    #     if isinstance(module, torch.nn.BatchNorm2d):\n",
    "    #         if hasattr(module, 'weight'):\n",
    "    #             module.weight.requires_grad_(False)\n",
    "    #         if hasattr(module, 'bias'):\n",
    "    #             module.bias.requires_grad_(False)\n",
    "    #         module.eval()\n",
    "    model = model.to(device)\n",
    "    # model=torch.compile(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_default_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os \n",
    "import random\n",
    "\n",
    "def read_img_file(f):\n",
    "    img = Image.open(f)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "\n",
    "_transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths_labels,):\n",
    "        self.image_paths_labels = image_paths_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths_labels[idx][0]\n",
    "        label = self.image_paths_labels[idx][1]\n",
    "        try:\n",
    "            img = read_img_file(img_path)\n",
    "            img = np.array(img)\n",
    "            # img = all_transforms[0](image=img)[\"image\"]\n",
    "            img = _transform(img)\n",
    "            return (img, label)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"error reading {img_path}\")\n",
    "\n",
    "\n",
    "image_paths_and_classes = []\n",
    "\n",
    "\n",
    "for file_name in os.listdir(\"./no_watermark/\"):\n",
    "    image_paths_and_classes.append((\"./no_watermark/\"+file_name, 0.0))\n",
    "\n",
    "for file_name in os.listdir(\"./watermark/\"):\n",
    "    image_paths_and_classes.append((\"./watermark/\"+file_name, 1.0))\n",
    "\n",
    "# random.shuffle(image_paths_and_classes)\n",
    "# training = image_paths_and_classes[:int(len(image_paths_and_classes)*0.8)]\n",
    "# testing = image_paths_and_classes[int(len(image_paths_and_classes)*0.8):]   #0.2\n",
    "\n",
    "# image_paths_and_classes=[]\n",
    "for file_name in os.listdir(\"./train_generated/no_watermark/\"):\n",
    "    image_paths_and_classes.append((\"./train_generated/no_watermark/\"+file_name, 0.0))\n",
    "\n",
    "for file_name in os.listdir(\"./train_generated/watermark/\"):\n",
    "    image_paths_and_classes.append((\"./train_generated/watermark/\"+file_name, 1.0))\n",
    "\n",
    "random.shuffle(image_paths_and_classes)\n",
    "training = image_paths_and_classes\n",
    "\n",
    "testing = []\n",
    "for file_name in os.listdir(\"./test/no_watermark/\"):\n",
    "    testing.append((\"./test/no_watermark/\"+file_name, 0.0))\n",
    "\n",
    "for file_name in os.listdir(\"./test/watermark/\"):\n",
    "    testing.append((\"./test/watermark/\"+file_name, 1.0))\n",
    "\n",
    "# training.extend(image_paths_and_classes[:int(len(image_paths_and_classes)*0.8)])\n",
    "# random.shuffle(training)\n",
    "\n",
    "# testing2 = image_paths_and_classes[int(len(image_paths_and_classes)*0.8):]   #0.2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_dataset = CustomDataset(training)\n",
    "test_dataset = CustomDataset(testing)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True,num_workers=2, prefetch_factor=4,pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=True,num_workers=2, prefetch_factor=4,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from statistics import mean\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch import autocast\n",
    "import optuna\n",
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(lr,trial=None,EPOCHS=15):\n",
    "    wandb.init(project=\"effnet_b0_watermark\", entity=\"qwertyforce\",reinit=True)\n",
    "\n",
    "    wandb.config.update({\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        })\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr) \n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    loss_train=[]\n",
    "    loss_test=[]\n",
    "    acc_test=[]\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        train_one_epoch(loss_train,optimizer,criterion,scaler)\n",
    "        test(loss_test,acc_test,criterion)\n",
    "        if wandb:\n",
    "            wandb.log({\"loss_train\": loss_train[-1],\"epoch\":epoch})\n",
    "            wandb.log({\"loss_test\": loss_test[-1],\"epoch\":epoch})\n",
    "            wandb.log({\"acc_test\": acc_test[-1],\"epoch\":epoch})\n",
    "        # if trial:\n",
    "        #     trial.report(mean(loss_test), epoch)\n",
    "        #     if trial.should_prune():\n",
    "        #         raise optuna.exceptions.TrialPruned()\n",
    "    return mean(loss_test)\n",
    "\n",
    "\n",
    "def train_one_epoch(loss_train,optimizer,criterion,scaler):\n",
    "    model.train()\n",
    "    temp_train_loss=[]\n",
    "    start = timer()\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model.forward(data)\n",
    "            loss = criterion(outputs,labels.unsqueeze(1))\n",
    "        \n",
    "        # outputs = model.forward(data)\n",
    "        # loss = criterion(outputs,labels.unsqueeze(1))\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        temp_train_loss.append(loss.item())\n",
    "        # if batch_idx % 10==0:\n",
    "    end = timer()\n",
    "    print(f\"epoch_training_time: {end - start}\") \n",
    "    loss_train.append(mean(temp_train_loss))\n",
    "    print(f\"Train: {mean(temp_train_loss)}\")\n",
    "\n",
    "            \n",
    "\n",
    "def test(loss_test,acc_test, criterion):\n",
    "    model.eval()\n",
    "    temp_loss=[]\n",
    "    predictions, true_labels = [], []\n",
    "    for batch_idx, (data, labels) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "          true_labels.extend(labels)\n",
    "          data, labels = data.to(device), labels.to(device)\n",
    "          outputs = model.forward(data)\n",
    "          loss = criterion(outputs,labels.unsqueeze(1))\n",
    "          temp_loss.append(loss.item())\n",
    "\n",
    "          outputs = torch.sigmoid(outputs).cpu().numpy()\n",
    "          outputs = np.round(outputs)\n",
    "          predictions.extend(outputs)\n",
    "\n",
    "\n",
    "    loss_test.append(mean(temp_loss))\n",
    "    acc_test.append(accuracy_score(true_labels,predictions))\n",
    "    print(f\"Test: {mean(temp_loss)}\")\n",
    "    # return mean(temp_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(0.001,EPOCHS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m1\u001b[39m\u001b[39m==\u001b[39m\u001b[39m2\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# def objective(trial):\n",
    "#     global model\n",
    "#     model = create_default_model()\n",
    "#     lr = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
    "#     mean_loss = train(lr)\n",
    "#     return mean_loss\n",
    "\n",
    "# study = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner())\n",
    "# study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optuna.visualization import plot_contour\n",
    "# from optuna.visualization import plot_edf\n",
    "# from optuna.visualization import plot_intermediate_values\n",
    "# from optuna.visualization import plot_optimization_history\n",
    "# from optuna.visualization import plot_parallel_coordinate\n",
    "# from optuna.visualization import plot_param_importances\n",
    "# from optuna.visualization import plot_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_slice(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(loss_train)\n",
    "# plt.plot(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_dataset = CustomDataset(testing2)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32,shuffle=True,num_workers=2, prefetch_factor=8,pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels= [], []\n",
    "raw_values=[]\n",
    "import numpy as np\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, labels) in enumerate(test_dataloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        outputs = model.forward(data)\n",
    "        outputs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        raw_values.extend(outputs.reshape(-1))\n",
    "        outputs = np.round(outputs)\n",
    "        predictions.extend(outputs)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        # loss = criterion(outputs,labels.unsqueeze(1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      2592\n",
      "         1.0       0.98      0.96      0.97      1959\n",
      "\n",
      "    accuracy                           0.97      4551\n",
      "   macro avg       0.97      0.97      0.97      4551\n",
      "weighted avg       0.97      0.97      0.97      4551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m1\u001b[39m\u001b[39m==\u001b[39m\u001b[39m2\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 381/165036 [01:20<8:06:07,  5.65it/s] /usr/local/lib/python3.9/dist-packages/PIL/Image.py:3074: DecompressionBombWarning: Image size (129957065 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "  1%|          | 976/165036 [03:11<7:31:58,  6.05it/s] "
     ]
    }
   ],
   "source": [
    "p = \"/media/qwertyforce/26fbdf65-ba8c-46bc-bbd9-bc503969e854/scenery_cx/scenery/public/images/\"\n",
    "model.eval()\n",
    "model=model.to(\"cuda\")\n",
    "# p=\"./test/no_watermark/\"\n",
    "res=[]\n",
    "all_outputs=[]\n",
    "for file_name in tqdm(os.listdir(p)):\n",
    "    img = read_img_file(p+file_name)\n",
    "    img=img.resize((512,512),Image.Resampling.NEAREST)\n",
    "    img = _transform(img).cuda()\n",
    "    img.unsqueeze_(0)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward(img)\n",
    "        outputs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        all_outputs.append(outputs[0][0])\n",
    "    if np.round(outputs[0][0]) == 1:\n",
    "        res.append((file_name,outputs[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, './model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import onnxruntime as onnx\n",
    "# model.eval()\n",
    "# model=model.to(\"cpu\")\n",
    "# x = torch.randn(1, 3, 512, 512, requires_grad=False).cpu()\n",
    "# # torch_out = model(x)\n",
    "# torch.onnx.export(model,                     # model being run\n",
    "#                   x,                            # model input (or a tuple for multiple inputs)\n",
    "#                   \"model.onnx\",              # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,           # store the trained parameter weights inside the model file\n",
    "#                   opset_version=12,             # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,     # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],      # the model's input names\n",
    "#                   output_names = ['output'],\n",
    "#                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "#                                 'output' : {0 : 'batch_size'}})    # the model's output names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import onnxruntime\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"model.pt\")\n",
    "# model=model.to(\"cpu\")\n",
    "# model.eval()\n",
    "\n",
    "# inp_arr = torch.randn(32,3,512,512)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out_1 = model(inp_arr)\n",
    "\n",
    "\n",
    "# sess_options = onnxruntime.SessionOptions()\n",
    "# sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "# sess_options.enable_cpu_mem_arena=False\n",
    "# session = onnxruntime.InferenceSession(\"./model.onnx\", sess_options, providers=['CPUExecutionProvider'])\n",
    "# out_2 = session.run([], {'input':input_arr.numpy()})[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
